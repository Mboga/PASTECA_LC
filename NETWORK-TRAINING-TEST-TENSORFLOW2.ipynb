{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout,ZeroPadding2D, Dropout,Concatenate,Conv2DTranspose,UpSampling2D\n",
    "from tensorflow.keras.layers import Activation, Reshape\n",
    "from tensorflow.keras.layers import Convolution2D, Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from numpy.random import seed\n",
    "import time\n",
    "seed(1234)\n",
    "#tf.test.is_gpu_available()\n",
    "#tf.config.list_physical_devices('CPU')\n",
    "\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "## data utility functions\n",
    "def to_categorical_4d(y, nc):\n",
    "    \"\"\"Convert a reclassed ground truth array to one-hot encoding\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- ground truth image\n",
    "    nc -- number of classes\n",
    "    \"\"\"\n",
    "    Y = np.zeros((y.shape[0],\n",
    "                  nc,\n",
    "                  y.shape[2],\n",
    "                  y.shape[3]),\n",
    "                  dtype=np.int32)\n",
    "    y=y.astype(np.int32)\n",
    "\n",
    "    for h in range(y.shape[0]):\n",
    "        for i in range(y.shape[2]):\n",
    "            for j in range(y.shape[3]):\n",
    "\n",
    "                if y[h, 0, i, j] != 0:\n",
    "                    Y[h, y[h, 0, i, j]-1, i, j] = 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return Y\n",
    "\n",
    "def swap_arr(arr):\n",
    "    \"\"\"\n",
    "    #swap axes to that of tf backend (channels last)\n",
    "    \"\"\"\n",
    "    val_1=np.swapaxes(arr,1,2)\n",
    "    val_2=np.swapaxes(val_1,2,3)\n",
    "    return val_2\n",
    "\n",
    "\n",
    "def mix_params(params):\n",
    "    \"\"\"Create a list of possible parameter combinations.\n",
    "\n",
    "    Keyword arguments:\n",
    "    params -- the group of parameter values to combine.\n",
    "    \"\"\"\n",
    "    paramset = []\n",
    "    for paramkey in params:\n",
    "        _ = []\n",
    "        for i in range(len(params[paramkey])):\n",
    "            _.append({paramkey: params[paramkey][i]})\n",
    "        paramset.append(_)\n",
    "\n",
    "    paramset = list(itertools.product(*paramset))\n",
    "\n",
    "    finalparams = []\n",
    "    for params in paramset:\n",
    "        _ = dict()\n",
    "        for param in params:\n",
    "            if len(_) == 0:\n",
    "                _ = param\n",
    "            else:\n",
    "                __ = _.copy()\n",
    "                __.update(param)\n",
    "                _ = __\n",
    "        finalparams.append(_)\n",
    "\n",
    "    return finalparams  \n",
    "\n",
    "def make_dir_paths(mypath):\n",
    "    if not os.path.isdir(mypath):\n",
    "        os.makedirs(mypath)\n",
    "        print(\"created path\")\n",
    "\n",
    "#fcn atrous ariginal  \n",
    "def add_common_layers(y):\n",
    "    \"\"\"\n",
    "    add relu activation and batch normalization\n",
    "    \"\"\"\n",
    "    y = Activation ('relu')(y)\n",
    "    y = BatchNormalization()(y) #should i define the axis for BN? what is the effect of not specifying the BN\n",
    "    return y\n",
    "\n",
    "def fcn_atr_orig(psize, nc, nb, weights_path=None):\n",
    "    \"\"\"\n",
    "    original fcn atrous architecture\n",
    "    weights_path--for pre-trainng, provide weights through the weights_path argument\n",
    "    psize--patch size\n",
    "    nc--number of classes\n",
    "    nb--number of input channels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    inp=Input(shape=(psize, psize,nb))\n",
    "\n",
    "    x_0 = ZeroPadding2D((4, 4))(inp)\n",
    "    x_0 = Conv2D(64, (5, 5), padding = 'valid', dilation_rate = (2,2))(x_0)\n",
    "    x_0 = add_common_layers(x_0)\n",
    "    x_0 = ZeroPadding2D((1, 1))(x_0)\n",
    "    x_0 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_0)\n",
    "\n",
    "    x_1 = ZeroPadding2D((4, 4))(x_0)\n",
    "    x_1 = Conv2D(64, (5, 5), padding = 'valid', dilation_rate = (2,2))(x_1)\n",
    "    x_1 = add_common_layers(x_1)\n",
    "    x_1 = ZeroPadding2D((1, 1))(x_1)\n",
    "    x_1 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_1)\n",
    "\n",
    "    x_2 = ZeroPadding2D((2, 2))(x_1)\n",
    "    x_2 = Conv2D(64, (3, 3), padding = 'valid', dilation_rate = (2,2))(x_2)\n",
    "    x_2 = add_common_layers(x_2)\n",
    "    x_2 = ZeroPadding2D((1, 1))(x_2)\n",
    "    x_2 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_2)\n",
    "\n",
    "    x_3 = ZeroPadding2D((2, 2))(x_2)\n",
    "    x_3 = Conv2D(64, (3, 3), padding = 'valid', dilation_rate = (2,2))(x_3)\n",
    "    x_3 = add_common_layers(x_3)\n",
    "    x_3 = ZeroPadding2D((1, 1))(x_3)\n",
    "    x_3 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_3)\n",
    "\n",
    "    x_4 = ZeroPadding2D((2, 2))(x_3)\n",
    "    x_4 = Conv2D(64, (3, 3), padding = 'valid', dilation_rate = (2,2))(x_4)\n",
    "    x_4 = add_common_layers(x_4)\n",
    "    x_4 = ZeroPadding2D((1, 1))(x_4)\n",
    "    x_4 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_4)\n",
    "\n",
    "    x_5 = ZeroPadding2D((2, 2))(x_4)\n",
    "    x_5 = Conv2D(64, (3, 3), padding = 'valid', dilation_rate = (2,2))(x_5)\n",
    "    x_5 = add_common_layers(x_5)\n",
    "    x_5 = ZeroPadding2D((1, 1))(x_5)\n",
    "    x_5 = MaxPooling2D(pool_size=(3, 3),strides=1)(x_5)\n",
    "\n",
    "    #concatenation layer\n",
    "    xc6= Concatenate(axis = 3)([x_0,x_1,x_2,x_3,x_4,x_5])\n",
    "\n",
    "    x7=Conv2D(nc, (1, 1))(xc6)\n",
    "    out_p = Activation(\"softmax\")(x7)\n",
    "\n",
    "    model = Model(inputs=inp,outputs=out_p)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "def unet(psize, nc, nb, pretrained_weights = None):\n",
    "    #original network \n",
    "    #using upsampling lauyers\n",
    "    #without dropout layers\n",
    "    #with batch normalization layers\n",
    "\n",
    "    input_tensor = Input(shape=(psize, psize,nb))\n",
    "\n",
    "    conv1 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_tensor)\n",
    "    conv1 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #conv1 128, conv2 64, conv3 32, conv4 16 conv5 8\n",
    "    #transpose convolutions\n",
    "    #input_shape=()\n",
    "    up6 = Conv2DTranspose(512, (3,3), strides=(2,2),activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "\n",
    "    #merge6 = Concatenate(axis = 3)([up6,conv4])\n",
    "    merge6 = Concatenate(axis = 3)([conv4,up6])\n",
    "    conv6 = Conv2D(512, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(256, (3,3),strides=(2,2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "    merge7 = Concatenate(axis = 3)([conv3,up7])\n",
    "    conv7 = Conv2D(256, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2DTranspose(128, (3,3), strides=(2,2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    merge8 = Concatenate(axis = 3)([conv2,up8])\n",
    "    conv8 = Conv2D(128, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2DTranspose(64, (3,3), strides=(2,2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "    merge9 = Concatenate(axis = 3)([conv1,up9])\n",
    "    conv9 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    conv9 = Conv2D(nc, (1,1), activation = 'softmax', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    model = Model(inputs = input_tensor, outputs = conv9)\n",
    "\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "# paths for loading the training data\n",
    "\n",
    "EXPT_ID=\"128_5epochs_UNET\" #unique id for your experiment\n",
    "\n",
    "root_path=\"E:/ACADEMICS/DEEP_LEARNING_TUTORIAL_PASTECA/DEEP_LEARNING_TUTORIAL_PASTECA/GOMA_1947\"\n",
    "patch_size = 128\n",
    "nc=6\n",
    "nb = 1 # the number of channels of the input image, for panchromatic band, it is 1, for rgb it is 3\n",
    "nb_epochs = 2 # number of epochs for training the model\n",
    "\n",
    "logs_folder=root_path+\"/RESULTS/\"+EXPT_ID+\"/FIGURES_TIME_LOGS\"\n",
    "make_dir_paths(logs_folder)\n",
    "\n",
    "weights_folder = root_path+\"/RESULTS/\"+EXPT_ID+\"/weights\"\n",
    "make_dir_paths(weights_folder)\n",
    "\n",
    "w8dir = weights_folder + \"/saved_weights.hdf5\"\n",
    "\n",
    "### load the training data\n",
    "t0 = time.time()\n",
    "\n",
    "#curated data\n",
    "\"adding path to the gray images dataset\"\n",
    "smpldir=root_path+\"/samples/training_goma.hdf5\"\n",
    "\n",
    "#reading the data\n",
    "with h5py.File(smpldir, \"r\") as f:\n",
    "    x_train = np.asarray(f[\"X_train\"][:1000,:,:,:])\n",
    "    x_val = np.asarray(f[\"X_val\"][:100,:,:,:])\n",
    "    y_train = np.asarray(f[\"y_train\"][:1000,:,:,:])\n",
    "    y_val = np.asarray(f[\"y_val\"][:100,:,:,:])\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Finished importing data after %.2f mins\" % ((t1-t0)/60.0)) \n",
    "\n",
    "# convert to onehot encoding\n",
    "t0 = time.time()\n",
    "\n",
    "y_train_u= to_categorical_4d(y_train[:,:,:128,:128],nc)\n",
    "y_val_u= to_categorical_4d(y_val[:,:,:128,:128],nc)\n",
    "\n",
    "#reshape to channels last ordering\n",
    "y_train_resh = swap_arr(y_train_u)\n",
    "y_val_resh = swap_arr(y_val_u)\n",
    "\n",
    "#reshaping the x patches too\n",
    "x_train_resh=swap_arr(x_train[:,:,:128,:128])\n",
    "x_val_resh=swap_arr(x_val[:,:,:128,:128])\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Finished preparing data after %.2f mins\" % ((t1-t0)/60.0)) \n",
    "\n",
    "#SAVE A PLOT OF THE CLASS FREQUENCY AND THE CLASS FREQUENCY\n",
    "txtcontent=\"\"\n",
    "\n",
    "np.unique(y_train, return_counts=True)\n",
    "messagetoprint=\"class distributions:\\n%s\" %list(np.unique(y_train, return_counts=True))\n",
    "txtcontent+=messagetoprint+\"\\n\\n\"\n",
    "\n",
    "\n",
    "#plot the class distributions\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.savefig(logs_folder+\"/class_frequency.png\")\n",
    "plt.show()\n",
    "\n",
    "##CLEAR THE MEMORY OF Y_TRAIN,Y_VAL,X_VAL AND X_TRAIN\n",
    "\n",
    "#clear memory of the raw training data\n",
    "del x_train\n",
    "del y_train\n",
    "del x_val\n",
    "del y_val\n",
    "del y_train_u\n",
    "del y_val_u\n",
    "\n",
    "# the number of pixels per class in your training set\n",
    "#save to csv\n",
    "#np.unique(y_train, return_counts=True)\n",
    "\n",
    "# train the model by loading the training data in batches\n",
    "#no data augmentation\n",
    "#datagen.fit(x_train)\n",
    "\n",
    "bestparams = {'lrate': 0.1, 'momentum': 0.8, 'lrdecay': 0.001}\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "sgd=SGD(lr=bestparams['lrate'],\n",
    "            decay=bestparams['lrdecay'], \n",
    "            momentum=bestparams['momentum'],\n",
    "            nesterov=True)\n",
    "\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "# training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_resh, y_train_resh))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100).batch(16)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val_resh, y_val_resh))\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "\n",
    "#model_unet=unet(patch_size,nc,nb)\n",
    "model_atr=fcn_atr_orig(patch_size,nc,nb)\n",
    "\n",
    "#in tf2, categorical crossentropy can be computed over multiple dimensions\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cca = tf.keras.metrics.CategoricalAccuracy()\n",
    "#model_unet.summary() #to view the network parameters \n",
    "\n",
    "model_atr.compile(loss=cce,\n",
    "                  optimizer=sgd,\n",
    "                  metrics=[cca])\n",
    "\n",
    "history = model_atr.fit(train_dataset,\n",
    "                         epochs = 200,\n",
    "                        validation_data=val_dataset)\n",
    "\n",
    "#Save the weights\n",
    "model_atr.save_weights(w8dir)\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "print(\"Finished training after %.2f mins\" % ((t1-t0)/60.0))\n",
    "\n",
    "messagetoprint=\"Finished training after %.2f mins\" % ((t1-t0)/60.0)\n",
    "#print(messagetoprint)\n",
    "txtcontent+=messagetoprint+\"\\n\"\n",
    "\n",
    "\"visualize the learned weights\"\n",
    "#summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel ('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.axis([-1,200,0.50,0.95])\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.savefig(logs_folder+\"/model_accuracy.png\")\n",
    "\n",
    "#visualise and close the figure\n",
    "#plt.show()\n",
    "#plt.clf()\n",
    "\n",
    "\n",
    "#summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.axis([-1,200,0.0,1])\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.savefig(logs_folder+\"/model_loss.png\")\n",
    "\n",
    "#visualise and close the figure\n",
    "#plt.show()\n",
    "#plt.clf()\n",
    "\n",
    "\n",
    "#CLEAR THE MEMORY OF THE TRAINING AND TEST DATA\n",
    "del x_train_resh\n",
    "del y_train_resh\n",
    "del x_val_resh\n",
    "del y_val_resh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n",
      "Finished testing after 0.06 mins\n"
     ]
    }
   ],
   "source": [
    "############################################test the model#####################################\n",
    "##IMAGE TESTING\n",
    "\n",
    "## functions for testing \n",
    "import subprocess, glob\n",
    "from random import seed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "seed(1234)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "WINDOW_SIZE=[256,256]\n",
    "#predictions=root_path+\"/preds/ATR_128/MINI_TILES/\"\n",
    "raw_files_pan = glob.glob(root_path+'/raw_tif_totest/*.tif')\n",
    "mini_preds_folder = root_path+\"/RESULTS/\"+EXPT_ID+\"/predictions/MINI_TILES\"\n",
    "probs_folder = root_path+\"/RESULTS/\"+EXPT_ID+\"/probabilities/WHOLE_TILE_PROBS\"\n",
    "\n",
    "make_dir_paths(probs_folder)\n",
    "make_dir_paths(mini_preds_folder)\n",
    "\n",
    "w8_fname = w8dir\n",
    "patch_size=256\n",
    "STRIDE=228\n",
    "nc=6\n",
    "\n",
    "#tiles to test\n",
    "gmax=63450\n",
    "gmin=0\n",
    "\n",
    "## functions for loading the data\n",
    "def nulltozero(arr):\n",
    "    arrcopy = np.copy(arr)\n",
    "    low_values_flags = arrcopy < 0 #0.7,0  \n",
    "    arrcopy[low_values_flags]=0\n",
    "    return arrcopy\n",
    "#functions for loading the images and converting them to arrays\n",
    "\n",
    "def img_to_array(*images):\n",
    "    \"\"\"Convert an image or list of images to numpy arrays.\n",
    "\n",
    "    Keyword arguments:\n",
    "    *images -- list containing the images to be converted\n",
    "    \"\"\"\n",
    "    imgarrays = []\n",
    "    i = 0\n",
    "    for img in images:\n",
    "        arr = gtiff_to_array(img)\n",
    "        imgarrays.append(arr)\n",
    "    return imgarrays\n",
    "\n",
    "\n",
    "def gtiff_to_array(imgfname):                                      \n",
    "    \"\"\"Transform a geotiff to numpy array.\n",
    "\n",
    "    Keyword arguments:\n",
    "    imgfnames -- filename of image to convert\n",
    "    \"\"\"\n",
    "    ds = gdal.Open(imgfname)\n",
    "    for band in range(ds.RasterCount):\n",
    "        band += 1\n",
    "        if band == 1:\n",
    "            arr = np.array(ds.GetRasterBand(band).ReadAsArray())\n",
    "            arr = np.expand_dims(arr, axis=2)\n",
    "        else:\n",
    "            concat = np.array(ds.GetRasterBand(band).ReadAsArray())\n",
    "            concat = np.expand_dims(concat, axis=2)\n",
    "            arr = np.concatenate((arr,\n",
    "                                  concat),\n",
    "                                 axis=2)\n",
    "    return arr\n",
    "\n",
    "def reclassgts2(gtsarray):\n",
    "    \"\"\"Reclassify ground truth dataset array to single class numbers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    gtsarray -- the ground truth dataset array\n",
    "    \"\"\"\n",
    "    reclassarray = np.zeros(shape=(gtsarray.shape[0], gtsarray.shape[1]),\n",
    "                            dtype=np.uint8)\n",
    "    cnum = 1\n",
    "    mask = np.logical_or(np.logical_or(gtsarray[:,:,0]==1,gtsarray[:,:,0]==3),gtsarray[:,:,0]==4)\n",
    "    reclassarray[mask]=cnum\n",
    "\n",
    "    cnum2 = 2\n",
    "    mask = np.logical_or(gtsarray[:,:,0]==2,gtsarray[:,:,0]==2)\n",
    "    reclassarray[mask] = cnum2\n",
    "    return reclassarray\n",
    "\n",
    "#create idxarray for the creation of the map\n",
    "\n",
    "def sample_idx(arr):\n",
    "    \"\"\"Randomly sample an array stratified based on frequency.\n",
    "\n",
    "    Keyword arguments:\n",
    "    arr -- the array being sampled\n",
    "    cratios -- the representative fractions of each classes\n",
    "    n -- total number of samples (default 1000)\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    arr_copy = np.copy(arr)\n",
    "    idxarray = np.zeros(shape=(0, 2), dtype=np.int16)\n",
    "    nc = np.array(np.where(arr_copy >0)).T.shape[0]\n",
    "    arr_flat = arr_copy.flatten()\n",
    "    n = arr_flat.shape[0]\n",
    "    randidx = np.asarray(range(n),dtype= np.int32)\n",
    "    #randidx=randidx.astype(np.int32)\n",
    "    idxarray = np.array(np.where(arr_copy >= 0)).T[randidx, :]\n",
    "    #sampleidx += csamples\n",
    "\n",
    "    del arr_copy\n",
    "    return idxarray\n",
    "\n",
    "def write_geotiff(fname, data, geo_transform, projection):\n",
    "    \"\"\"Create a GeoTIFF file with the given data.\"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    dataset = None  # Close the file\n",
    "\n",
    "def swap_axes(arr):\n",
    "    return np.expand_dims(arr, axis = 0)\n",
    "\n",
    "##attempting to perform classification by loading the entire tiles\n",
    "\n",
    "#remove nan\n",
    "def nulltozero_g(arr):\n",
    "    arrcopy = np.copy(arr)\n",
    "    low_values_flags = (np.isnan(arr))#0.7,0\n",
    "    arrcopy[low_values_flags]=0\n",
    "    return arrcopy\n",
    "\n",
    "def extract_geometry(path_dir):\n",
    "    raster_dataset = gdal.Open(path_dir, gdal.GA_ReadOnly)\n",
    "    geo_transform = raster_dataset.GetGeoTransform()\n",
    "\n",
    "    return geo_transform\n",
    "\n",
    "# Utils adapting the prediction scheme in https://github.com/nshaud/DeepNetsForEO/blob/master/SegNet_PyTorch_v2.ipynb\n",
    "#https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "\n",
    "\n",
    "def get_random_pos(img, window_shape):\n",
    "    \"\"\" Extract of 2D random patch of shape window_shape in the image \"\"\"\n",
    "    w, h = window_shape\n",
    "    W, H = img.shape[-2:]\n",
    "    x1 = random.randint(0, W - w - 1)\n",
    "    x2 = x1 + w\n",
    "    y1 = random.randint(0, H - h - 1)\n",
    "    y2 = y1 + h\n",
    "    return x1, x2, y1, y2\n",
    "\n",
    "def accuracy(input, target):\n",
    "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
    "\n",
    "def sliding_window(top, step=10, window_size=(32,32)):\n",
    "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            yield x, y, window_size[0], window_size[1]\n",
    "\n",
    "def count_sliding_window(top, step=10, window_size=(32,32)):\n",
    "    \"\"\" Count the number of windows in an image \"\"\"\n",
    "    c = 0\n",
    "    for x in range(0, top.shape[0], step):\n",
    "        if x + window_size[0] > top.shape[0]:\n",
    "            x = top.shape[0] - window_size[0]\n",
    "        for y in range(0, top.shape[1], step):\n",
    "            if y + window_size[1] > top.shape[1]:\n",
    "                y = top.shape[1] - window_size[1]\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def grouper(n, iterable):\n",
    "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "# enable the testing of the entire test set\n",
    "def test(net, img_arr,N_CLASSES,stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
    "    \n",
    "    # Use the network on the test set\n",
    "    all_preds = []\n",
    "    all_probs=[]\n",
    "    all_gts = []\n",
    "    \n",
    "    for img in img_arr:\n",
    "        #print (img.shape)\n",
    "        pred = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
    "        prob = np.zeros(img.shape[:2] + (N_CLASSES,))\n",
    "        for i, coords in enumerate(grouper(batch_size, sliding_window(img, step=stride, window_size=window_size))):\n",
    "            image_patches_1=[]\n",
    "            image_patches = [np.copy(np.expand_dims(img[x:x+w, y:y+h],axis=0)) for x,y,w,h in coords]\n",
    "            image_patches_1= np.concatenate(image_patches, axis=0)\n",
    "            # Do the inference\n",
    "            outs= net.predict(image_patches_1)\n",
    "            del image_patches # clear mem\n",
    "\n",
    "            # Fill in the results array\n",
    "            for out, (x, y, w, h) in zip(outs, coords):\n",
    "                #out = out.transpose((1,2,0))\n",
    "                #pred[x:x+w, y:y+h] += out  #this creates addition in the overlap regions\n",
    "\n",
    "                pred[x:x+w, y:y+h] = out\n",
    "            del(outs)\n",
    "        \"use the np max for max probability\"\n",
    "        prob = np.max(pred, axis=2)\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        \n",
    "        all_preds.append(pred)\n",
    "        all_probs.append(prob)\n",
    "\n",
    "    return all_preds,all_probs\n",
    "\n",
    "#Reclassifying the array and giving it coordinates\n",
    "def reclass_gts(gtsarray):\n",
    "    \"\"\"Reclassify ground truth dataset array to single class numbers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    gtsarray -- the ground truth dataset array\n",
    "    \"\"\"\n",
    "    reclassarray = np.zeros(shape=(gtsarray.shape[0], gtsarray.shape[1]),\n",
    "                            dtype=np.uint8)\n",
    "    cnum = 1\n",
    "    for color in _ccolors:\n",
    "        mask = np.logical_and(np.logical_and(gtsarray[:, :, 0] == color[0],\n",
    "                              gtsarray[:, :, 1] == color[1]),\n",
    "                              gtsarray[:, :, 2] == color[2])\n",
    "        reclassarray[mask] = cnum\n",
    "        cnum += 1\n",
    "    return reclassarray\n",
    "\n",
    "def save_to_1band(arr,out_path,geom,proj):\n",
    "    #save a one band image\n",
    "    #p_rec is the reclassified array and has two dimensions\n",
    "    #pred_map_GCs2 = predsdir + \"trainingset_3/georeferenced/fcn_atr_goma_1947_clip1_1bands.tif\"\n",
    "    arr=np.expand_dims(arr, axis=2)\n",
    "    nrows,ncols,nbands = arr.shape[0],arr.shape[1],arr.shape[2]\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    nw_ds = driver.Create(out_path, ncols, nrows, nbands, gdal.GDT_UInt32 )\n",
    "    nw_ds.SetGeoTransform(geom)\n",
    "    nw_ds.SetProjection(proj)\n",
    "\n",
    "    nw_ds.GetRasterBand(1).WriteArray(arr[:, :,0])\n",
    "\n",
    "    #for i in range(nbands):\n",
    "    #\tnw_ds.GetRasterBand(i+1).WriteArray(P_rec[:, :, i])\n",
    "\n",
    "    nw_ds = None\n",
    "\n",
    "def save_prob_1band(arr,out_path,geom,proj):\n",
    "    #save a one band image\n",
    "    #p_rec is the reclassified array and has two dimensions\n",
    "    #pred_map_GCs2 = predsdir + \"trainingset_3/georeferenced/fcn_atr_goma_1947_clip1_1bands.tif\"\n",
    "    arr=np.expand_dims(arr, axis=2)\n",
    "    nrows,ncols,nbands = arr.shape[0],arr.shape[1],arr.shape[2]\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    nw_ds = driver.Create(out_path, ncols, nrows, nbands, gdal.GDT_Float32 )\n",
    "    nw_ds.SetGeoTransform(geom)\n",
    "    nw_ds.SetProjection(proj)\n",
    "\n",
    "    nw_ds.GetRasterBand(1).WriteArray(arr[:, :,0])\n",
    "\n",
    "    #for i in range(nbands):\n",
    "    #\tnw_ds.GetRasterBand(i+1).WriteArray(P_rec[:, :, i])\n",
    "\n",
    "    nw_ds = None\n",
    "\n",
    "# normalize the top array\n",
    "def norm_rgbn(data,gmax,gmin):\n",
    "    \"\"\"\n",
    "    rexcale  the multispectral data [0,1]\n",
    "    data--the multispectral array\n",
    "    \"\"\"\n",
    "    data = data.astype(float)\n",
    "    data_norm = (data - gmin)/(gmax - gmin)\n",
    "    return data_norm\n",
    "\n",
    "def data_proj(arr_path):\n",
    "    raster_dataset = gdal.Open(arr_path, gdal.GA_ReadOnly)\n",
    "    geo_transform = raster_dataset.GetGeoTransform()\n",
    "    proj = raster_dataset.GetProjectionRef()\n",
    "    return geo_transform,proj\n",
    "\n",
    "\n",
    "#call the trained model\n",
    "t0 = time.time()\n",
    "\n",
    "#model_fcn=fcn_atr_orig(patch_size,nc,1)\n",
    "model_unet=unet(patch_size,nc,nb)\n",
    "\n",
    "#load the weights\n",
    "model_unet.load_weights(w8_fname)\n",
    "for rgbn in raw_files_pan:\n",
    "    rgbn_raw=img_to_array(rgbn)\n",
    "    geo_info,projec_info=data_proj(rgbn)\n",
    "\n",
    "    #normalize the rgb\n",
    "    rgbn_norm=norm_rgbn(rgbn_raw[0],gmax,gmin)\n",
    "    del rgbn_raw\n",
    "\n",
    "    allpred= test(model_unet,[rgbn_norm],nc,stride=STRIDE)\n",
    "\n",
    "    file_name=os.path.split(rgbn)[-1]\n",
    "    save_to_1band(allpred[0][0]+1,mini_preds_folder+\"/\"+file_name,geo_info,projec_info)\n",
    "    save_prob_1band(allpred[1][0],probs_folder+\"/\"+file_name,geo_info,projec_info)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Finished testing after %.2f mins\" % ((t1-t0)/60.0))\n",
    "\n",
    "#txtcontent=\"\"\n",
    "\n",
    "messagetoprint=\"Finished testing after %.2f mins\" % ((t1-t0)/60.0)\n",
    "#print(messagetoprint)\n",
    "txtcontent+=messagetoprint+\"\\n\\n\"\n",
    "\n",
    "f = open(logs_folder+\"/model_training_testing.txt\", 'w')\n",
    "f.write(EXPT_ID+\" time_log\"+\"\\n\\n\")\n",
    "f.write(txtcontent)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFY THE ENTIRE STUDY AREA\n",
    "#This is the path to the image covering your study area\n",
    "raw_files_large=[\"H:/PASTECA/DATA/GOMA_1947/Goma1947_UTM35S_ortho_1m_test.tif\"]\n",
    "t0 = time.time()\n",
    "\n",
    "large_preds_folder = root_path+\"/RESULTS/\"+EXPT_ID+\"/predictions/WHOLE_TILE\"\n",
    "probs_folder = root_path+\"/RESULTS/\"+EXPT_ID+\"/probabilities/WHOLE_TILE_PROBS\"\n",
    "make_dir_paths(large_preds_folder)\n",
    "make_dir_paths(probs_folder)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "for rgbn in raw_files_large:\n",
    "    rgbn_raw=img_to_array(rgbn)\n",
    "    geo_info,projec_info=data_proj(rgbn)\n",
    "\n",
    "    #normalize the rgb\n",
    "    rgbn_norm=norm_rgbn(rgbn_raw[0],gmax,gmin)\n",
    "    del rgbn_raw\n",
    "\n",
    "    allpred= test(model_unet,[rgbn_norm],nc,stride=228)\n",
    "\n",
    "    file_name=os.path.split(rgbn)[-1]\n",
    "    save_to_1band(allpred[0][0]+1,large_preds_folder+\"/\"+file_name,geo_info,projec_info)\n",
    "    save_prob_1band(allpred[1][0],probs_folder+\"/\"+file_name,geo_info,projec_info)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Finished testing whole tile after %.2f mins\" % ((t1-t0)/60.0))\n",
    "txtcontent=\"\"\n",
    "messagetoprint=\"Finished testing whole tile after %.2f mins\" % ((t1-t0)/60.0)\n",
    "#print(messagetoprint)\n",
    "txtcontent+=messagetoprint+\"\\n\\n\"\n",
    "\n",
    "f = open(logs_folder+\"/testing_large_file.txt\", 'w')\n",
    "f.write(EXPT_ID+\" testing_time\"+\"\\n\\n\")\n",
    "f.write(txtcontent)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
